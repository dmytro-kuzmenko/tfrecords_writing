{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0i7ADd1Vdec",
        "outputId": "ac504bbf-c28c-454d-b0f8-28a6701dd509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "project_id = 'stairnet-unlabeled'\n",
        "# !gcloud auth login\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "from google.colab import drive, auth\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpS8gDMkgKnK"
      },
      "source": [
        "## Load tfrecords data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qAJuaTgHqlJ"
      },
      "outputs": [],
      "source": [
        "GCS_PATH = \"gs://bucket_name/tfrecords_out_folder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFTrdEVCH3T3"
      },
      "outputs": [],
      "source": [
        "UNLABELED_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/_training_01of01.tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHI76PtCZzXy"
      },
      "outputs": [],
      "source": [
        "# List all the TFRecords and create a dataset from it\n",
        "filenames_dataset = tf.data.Dataset.from_tensor_slices(UNLABELED_FILENAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfDjYoAaGmaw"
      },
      "outputs": [],
      "source": [
        "# Create a description of the features.\n",
        "feature_description = {\n",
        "    'image': tf.io.FixedLenFeature([], tf.string),\n",
        "    'label': tf.io.FixedLenSequenceFeature([], tf.string, allow_missing=True)\n",
        "}\n",
        "\n",
        "@tf.function\n",
        "def _parse_function(example_proto):\n",
        "    # Parse the input `tf.Example` proto using the dictionary above.\n",
        "    return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "# Preprocess Image\n",
        "@tf.function\n",
        "def process_image_tfrecord(record):  \n",
        "    image = tf.io.decode_jpeg(record['image'], channels=3)\n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = tf.reshape(image, [224, 224, 3])\n",
        "    # image = tf.image.random_crop(value=image, size=(224, 224, 3))\n",
        "    label = record['label']\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "# Create a Dataset composed of TFRecords (paths to bucket)\n",
        "@tf.function\n",
        "def get_tfrecord(filename):\n",
        "    return tf.data.TFRecordDataset(filename, num_parallel_reads=AUTO)\n",
        "\n",
        "def build_dataset(dataset):\n",
        "\n",
        "    dataset = dataset.interleave(get_tfrecord, num_parallel_calls=AUTO)\n",
        "    \n",
        "    # Transformation: IO Intensive \n",
        "    dataset = dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
        "\n",
        "    # Transformation: CPU Intensive\n",
        "    dataset = dataset.map(process_image_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "    if repeat:\n",
        "        dataset = dataset.repeat()\n",
        "\n",
        "    dataset = dataset.batch(batch_size=batch_size)\n",
        "    dataset = dataset.shuffle(30000) # sample_size // 10\n",
        "    dataset = dataset.cache()\n",
        "\n",
        "    # Pipeline next iteration\n",
        "    dataset = dataset.prefetch(buffer_size=AUTO)\n",
        "    \n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = build_dataset(filenames_dataset)"
      ],
      "metadata": {
        "id": "O5Mr0K1EzIlt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "dQPNVntiOdEs",
        "xqXFEOCdY7CP",
        "Gr2BlgsWaGD0",
        "7_9oalxRnp24"
      ],
      "machine_shape": "hm",
      "name": "Load TFRecords",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}